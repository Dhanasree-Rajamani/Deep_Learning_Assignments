{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3/gH1tUcm7ceEn7JV1Gw6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanasree-Rajamani/Deep_Learning_Assignments/blob/main/Assignment_3/Pytorch_Lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab consists of:  \n",
        "\n",
        "- Designing the model with Pytorch Lightning \n",
        "\n",
        "- This Neural network model consists of 4 layers : 1 input, 2 hidden and 1 output"
      ],
      "metadata": {
        "id": "6NhmtamoOQyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Pytorch Lightning"
      ],
      "metadata": {
        "id": "cVKzOOJKfeEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d76U22rnbgU_",
        "outputId": "42601b68-ae24-405e-d1b7-d5cfde32a2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.8/dist-packages (1.9.4)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (2023.1.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (23.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (0.11.3)\n",
            "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (0.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.13.1+cu116)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.25.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "X2jV9ub5AsXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are setting the model here:\n",
        "\n",
        "model consists of 4 layers : 1 input layer, 2 hidden layers and 1 output layer\n",
        "\n",
        "input is 28*28 handwritten digit images\n",
        "\n",
        "since digits are 0-9, we have 10 output labels\n"
      ],
      "metadata": {
        "id": "CH_qHtuHfi7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MNISTClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MNISTClassifier, self).__init__()\n",
        "\n",
        "    # mnist images are (1, 28, 28) (channels, width, height) \n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 128)\n",
        "    self.layer_3 = torch.nn.Linear(128, 256)\n",
        "    self.layer_4 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, width, height = x.size()\n",
        "\n",
        "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "    x = x.view(batch_size, -1)\n",
        "\n",
        "    # layer 1\n",
        "    x = self.layer_1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 2\n",
        "    x = self.layer_2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 3\n",
        "    x = self.layer_3(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 4\n",
        "    x = self.layer_4(x)\n",
        "\n",
        "    # probability distribution over labels\n",
        "    x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "sPyrmKrgb4RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing the model with Pytorch Lightning\n",
        "\n",
        "This Neural network model consists of 4 layers : 1 input, 2 hidden and 1 output"
      ],
      "metadata": {
        "id": "1O2PII6LhR6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNRUhzNwTBg1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LightningMNISTClassifier, self).__init__()\n",
        "\n",
        "    # mnist images are (1, 28, 28) (channels, width, height) \n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 128)\n",
        "    self.layer_3 = torch.nn.Linear(128, 256)\n",
        "    self.layer_4 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, width, height = x.siz()\n",
        "\n",
        "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "    x = x.view(batch_size, -1)\n",
        "\n",
        "    # layer 1\n",
        "    x = self.layer_1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 2\n",
        "    x = self.layer_2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 3\n",
        "    x = self.layer_3(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 4\n",
        "    x = self.layer_4(x)\n",
        "\n",
        "    # probability distribution over labels\n",
        "    x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting The Data\n",
        "\n",
        "We split MNIST data to a training, validation and test split.\n",
        "\n",
        "The dataset is added to the Dataloader which handles the loading, shuffling and batching of the dataset.\n",
        "\n",
        "Data preparation:\n",
        "- Image transforms \n",
        "- Generate training, validation and test dataset splits.\n",
        "- Wrap each dataset split in a DataLoader"
      ],
      "metadata": {
        "id": "hwcAZ9ODhjD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# ----------------\n",
        "# TRANSFORMS\n",
        "# ----------------\n",
        "# prepare transforms standard to MNIST\n",
        "transform=transforms.Compose([transforms.ToTensor(), \n",
        "                              transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# ----------------\n",
        "# TRAINING, VAL DATA\n",
        "# ----------------\n",
        "mnist_train = MNIST(os.getcwd(), train=True, download=True)\n",
        "\n",
        "# train (55,000 images), val split (5,000 images)\n",
        "mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "\n",
        "# ----------------\n",
        "# TEST DATA\n",
        "# ----------------\n",
        "mnist_test = MNIST(os.getcwd(), train=False, download=True)\n",
        "\n",
        "# ----------------\n",
        "# DATALOADERS\n",
        "# ----------------\n",
        "# The dataloaders handle shuffling, batching, etc...\n",
        "mnist_train = DataLoader(mnist_train, batch_size=64)\n",
        "mnist_val = DataLoader(mnist_val, batch_size=64)\n",
        "mnist_test = DataLoader(mnist_test, batch_size=64)"
      ],
      "metadata": {
        "id": "SVHR3ZqNbaDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Laoding the data using the following methods: \n",
        "- train_dataloader()\n",
        "- val_dataloader()\n",
        "- test_dataloader()\n",
        "\n",
        "Method for data preparation: \n",
        "- prepare_data()"
      ],
      "metadata": {
        "id": "eLGmL17Kh5CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def setup(self, stage):\n",
        "    # transforms for images\n",
        "    transform=transforms.Compose([transforms.ToTensor(), \n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
        "      \n",
        "    # prepare transforms standard to MNIST\n",
        "    mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
        "    mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\n",
        "    \n",
        "    self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.mnist_train, batch_size=64)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.mnist_val, batch_size=64)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self,mnist_test, batch_size=64)"
      ],
      "metadata": {
        "id": "UaxTWjYsbukm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Optimizer\n",
        "\n",
        "Using Adam optimizer for optimization.\n",
        "\n",
        "The optimizer is given the weights to optimizer when we init the optimizer.\n",
        "\n",
        "The optimizer code is added to the function configure_optimizers() in the LightningModule."
      ],
      "metadata": {
        "id": "keSSvA-RiEba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "      return optimizer"
      ],
      "metadata": {
        "id": "-400Vx7PeBeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss\n",
        "\n",
        "In this case, we want to take our logits and calculate the cross entropy loss. Since cross entropy is the same as NegativeLogLikelihood(log_softmax), we just need to add the nll_loss.\n"
      ],
      "metadata": {
        "id": "oYaSpDvUiIMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "def cross_entropy_loss(logits, labels):\n",
        "  return F.nll_loss(logits, labels)"
      ],
      "metadata": {
        "id": "RyIEsZneeC-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch Lightning, calculating loss as follows."
      ],
      "metadata": {
        "id": "8-6AzBJUiQ_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "    return F.nll_loss(logits, labels)"
      ],
      "metadata": {
        "id": "Fcc_QmQGeE7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Training loop \n",
        "\n",
        "Determine training and validation loss"
      ],
      "metadata": {
        "id": "GDWTwVP3iWAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "# -----------------\n",
        "# MODEL\n",
        "# -----------------\n",
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LightningMNISTClassifier, self).__init__()\n",
        "\n",
        "    # mnist images are (1, 28, 28) (channels, width, height) \n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 128)\n",
        "    self.layer_3 = torch.nn.Linear(128, 256)\n",
        "    self.layer_4 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, width, height = x.sizes()\n",
        "\n",
        "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "    x = x.view(batch_size, -1)\n",
        "\n",
        "    # layer 1\n",
        "    x = self.layer_1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 2\n",
        "    x = self.layer_2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 3\n",
        "    x = self.layer_3(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 4\n",
        "    x = self.layer_4(x)\n",
        "\n",
        "    # probability distribution over labels\n",
        "    x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "# ----------------\n",
        "# DATA\n",
        "# ----------------\n",
        "transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
        "mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\n",
        "\n",
        "# train (55,000 images), val split (5,000 images)\n",
        "mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "mnist_test = MNIST(os.getcwd(), train=False, download=True)\n",
        "\n",
        "# The dataloaders handle shuffling, batching, etc...\n",
        "mnist_train = DataLoader(mnist_train, batch_size=64)\n",
        "mnist_val = DataLoader(mnist_val, batch_size=64)\n",
        "mnist_test = DataLoader(mnist_test, batch_size=64)\n",
        "\n",
        "# ----------------\n",
        "# OPTIMIZER\n",
        "# ----------------\n",
        "pytorch_model = MNISTClassifier()\n",
        "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=1e-3)\n",
        "\n",
        "# ----------------\n",
        "# LOSS\n",
        "# ----------------\n",
        "def cross_entropy_loss(logits, labels):\n",
        "  return F.nll_loss(logits, labels)\n",
        "\n",
        "# ----------------\n",
        "# TRAINING LOOP\n",
        "# ----------------\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # TRAINING LOOP\n",
        "  for train_batch in mnist_train:\n",
        "    x, y = train_batch\n",
        "\n",
        "    logits = pytorch_model(x)\n",
        "    loss = cross_entropy_loss(logits, y)\n",
        "    print('train loss: ', loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  # VALIDATION LOOP\n",
        "  with torch.no_grad():\n",
        "    val_loss = []\n",
        "    for val_batch in mnist_val:\n",
        "      x, y = val_batch\n",
        "      logits = pytorch_model(x)\n",
        "      val_loss.append(cross_entropy_loss(logits, y).item())\n",
        "\n",
        "    val_loss = torch.mean(torch.tensor(val_loss))\n",
        "    print('val_loss: ', val_loss.item())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6isRz8NeIh9",
        "outputId": "1579ce6d-9d61-43db-d05c-1c745d88df37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:  2.2945244312286377\n",
            "train loss:  2.268749475479126\n",
            "train loss:  2.2636992931365967\n",
            "train loss:  2.19991397857666\n",
            "train loss:  2.224827766418457\n",
            "train loss:  2.097792625427246\n",
            "train loss:  2.0646939277648926\n",
            "train loss:  2.069551467895508\n",
            "train loss:  1.9330109357833862\n",
            "train loss:  1.8593388795852661\n",
            "train loss:  1.7185806035995483\n",
            "train loss:  1.673564076423645\n",
            "train loss:  1.474409580230713\n",
            "train loss:  1.4042366743087769\n",
            "train loss:  1.3478622436523438\n",
            "train loss:  1.2908762693405151\n",
            "train loss:  1.3241838216781616\n",
            "train loss:  0.964457094669342\n",
            "train loss:  1.1541366577148438\n",
            "train loss:  1.035294771194458\n",
            "train loss:  0.9673970937728882\n",
            "train loss:  0.9156351685523987\n",
            "train loss:  0.7617364525794983\n",
            "train loss:  0.8678506016731262\n",
            "train loss:  0.9399540424346924\n",
            "train loss:  0.5769922733306885\n",
            "train loss:  1.0428754091262817\n",
            "train loss:  0.7471843957901001\n",
            "train loss:  0.6398500204086304\n",
            "train loss:  0.8257083296775818\n",
            "train loss:  0.6847691535949707\n",
            "train loss:  0.5905737280845642\n",
            "train loss:  0.7410441637039185\n",
            "train loss:  0.670600950717926\n",
            "train loss:  0.4841228723526001\n",
            "train loss:  0.4642517864704132\n",
            "train loss:  0.5647309422492981\n",
            "train loss:  0.51479172706604\n",
            "train loss:  0.6008332967758179\n",
            "train loss:  0.5147079825401306\n",
            "train loss:  0.47983458638191223\n",
            "train loss:  0.47204360365867615\n",
            "train loss:  0.6030774116516113\n",
            "train loss:  0.33204296231269836\n",
            "train loss:  0.9156526923179626\n",
            "train loss:  0.42836737632751465\n",
            "train loss:  0.3892238140106201\n",
            "train loss:  0.5746538639068604\n",
            "train loss:  0.47309958934783936\n",
            "train loss:  0.5833832025527954\n",
            "train loss:  0.6465615630149841\n",
            "train loss:  0.23087866604328156\n",
            "train loss:  0.4665553569793701\n",
            "train loss:  0.5407358407974243\n",
            "train loss:  0.4130252003669739\n",
            "train loss:  0.4522922933101654\n",
            "train loss:  0.3700612783432007\n",
            "train loss:  0.39396730065345764\n",
            "train loss:  0.3418438136577606\n",
            "train loss:  0.7371364831924438\n",
            "train loss:  0.33337822556495667\n",
            "train loss:  0.3580859303474426\n",
            "train loss:  0.43462908267974854\n",
            "train loss:  0.3798586428165436\n",
            "train loss:  0.5237574577331543\n",
            "train loss:  0.3411591947078705\n",
            "train loss:  0.395330548286438\n",
            "train loss:  0.34164825081825256\n",
            "train loss:  0.5209336280822754\n",
            "train loss:  0.37415897846221924\n",
            "train loss:  0.4396318793296814\n",
            "train loss:  0.35118916630744934\n",
            "train loss:  0.43505313992500305\n",
            "train loss:  0.28274431824684143\n",
            "train loss:  0.3909052610397339\n",
            "train loss:  0.3101889193058014\n",
            "train loss:  0.529973030090332\n",
            "train loss:  0.20957183837890625\n",
            "train loss:  0.18588653206825256\n",
            "train loss:  0.37375083565711975\n",
            "train loss:  0.41263556480407715\n",
            "train loss:  0.4475706219673157\n",
            "train loss:  0.5016743540763855\n",
            "train loss:  0.35447511076927185\n",
            "train loss:  0.41234779357910156\n",
            "train loss:  0.46327969431877136\n",
            "train loss:  0.3787282705307007\n",
            "train loss:  0.45268651843070984\n",
            "train loss:  0.34443986415863037\n",
            "train loss:  0.10645856708288193\n",
            "train loss:  0.20463882386684418\n",
            "train loss:  0.3909294605255127\n",
            "train loss:  0.5549992918968201\n",
            "train loss:  0.37008604407310486\n",
            "train loss:  0.22004026174545288\n",
            "train loss:  0.44191595911979675\n",
            "train loss:  0.41614437103271484\n",
            "train loss:  0.23455747961997986\n",
            "train loss:  0.3800671100616455\n",
            "train loss:  0.4055868983268738\n",
            "train loss:  0.6626355051994324\n",
            "train loss:  0.3684352934360504\n",
            "train loss:  0.2422344833612442\n",
            "train loss:  0.24380306899547577\n",
            "train loss:  0.2797507643699646\n",
            "train loss:  0.3799036145210266\n",
            "train loss:  0.4903257489204407\n",
            "train loss:  0.274478554725647\n",
            "train loss:  0.20364417135715485\n",
            "train loss:  0.3537503778934479\n",
            "train loss:  0.5622830986976624\n",
            "train loss:  0.6766456961631775\n",
            "train loss:  0.2623884081840515\n",
            "train loss:  0.343558132648468\n",
            "train loss:  0.21443451941013336\n",
            "train loss:  0.43513205647468567\n",
            "train loss:  0.33704283833503723\n",
            "train loss:  0.2790330946445465\n",
            "train loss:  0.2776636481285095\n",
            "train loss:  0.20449167490005493\n",
            "train loss:  0.4035288393497467\n",
            "train loss:  0.3754141330718994\n",
            "train loss:  0.4393933117389679\n",
            "train loss:  0.5631971955299377\n",
            "train loss:  0.5022258758544922\n",
            "train loss:  0.43936407566070557\n",
            "train loss:  0.23061901330947876\n",
            "train loss:  0.3468417227268219\n",
            "train loss:  0.2954556941986084\n",
            "train loss:  0.41679421067237854\n",
            "train loss:  0.359988272190094\n",
            "train loss:  0.4522029459476471\n",
            "train loss:  0.39527538418769836\n",
            "train loss:  0.4135894775390625\n",
            "train loss:  0.3442543148994446\n",
            "train loss:  0.3114520311355591\n",
            "train loss:  0.17062924802303314\n",
            "train loss:  0.19826917350292206\n",
            "train loss:  0.22916513681411743\n",
            "train loss:  0.2896031141281128\n",
            "train loss:  0.2600947618484497\n",
            "train loss:  0.18780617415905\n",
            "train loss:  0.3577411472797394\n",
            "train loss:  0.32941776514053345\n",
            "train loss:  0.23882071673870087\n",
            "train loss:  0.4704256057739258\n",
            "train loss:  0.2989208996295929\n",
            "train loss:  0.6256957054138184\n",
            "train loss:  0.1471390426158905\n",
            "train loss:  0.13915792107582092\n",
            "train loss:  0.5128687024116516\n",
            "train loss:  0.37022557854652405\n",
            "train loss:  0.20573237538337708\n",
            "train loss:  0.28197893500328064\n",
            "train loss:  0.4905722737312317\n",
            "train loss:  0.44133731722831726\n",
            "train loss:  0.27943792939186096\n",
            "train loss:  0.34867364168167114\n",
            "train loss:  0.29901841282844543\n",
            "train loss:  0.2954639494419098\n",
            "train loss:  0.46601489186286926\n",
            "train loss:  0.48190298676490784\n",
            "train loss:  0.27935123443603516\n",
            "train loss:  0.29970741271972656\n",
            "train loss:  0.22780896723270416\n",
            "train loss:  0.46656152606010437\n",
            "train loss:  0.27064669132232666\n",
            "train loss:  0.4566061496734619\n",
            "train loss:  0.2667481303215027\n",
            "train loss:  0.2526765763759613\n",
            "train loss:  0.23634874820709229\n",
            "train loss:  0.1871362328529358\n",
            "train loss:  0.28780657052993774\n",
            "train loss:  0.45683586597442627\n",
            "train loss:  0.26520663499832153\n",
            "train loss:  0.29454776644706726\n",
            "train loss:  0.2545865476131439\n",
            "train loss:  0.18074950575828552\n",
            "train loss:  0.28101271390914917\n",
            "train loss:  0.3068498969078064\n",
            "train loss:  0.22735953330993652\n",
            "train loss:  0.34424087405204773\n",
            "train loss:  0.3138936161994934\n",
            "train loss:  0.2020602822303772\n",
            "train loss:  0.19298133254051208\n",
            "train loss:  0.14854376018047333\n",
            "train loss:  0.4272421598434448\n",
            "train loss:  0.21752330660820007\n",
            "train loss:  0.26985716819763184\n",
            "train loss:  0.3552163541316986\n",
            "train loss:  0.08515558391809464\n",
            "train loss:  0.35559752583503723\n",
            "train loss:  0.2005651742219925\n",
            "train loss:  0.23801881074905396\n",
            "train loss:  0.3811560571193695\n",
            "train loss:  0.14093013107776642\n",
            "train loss:  0.44464364647865295\n",
            "train loss:  0.20611345767974854\n",
            "train loss:  0.2084529995918274\n",
            "train loss:  0.25172561407089233\n",
            "train loss:  0.16829463839530945\n",
            "train loss:  0.09748274087905884\n",
            "train loss:  0.21555785834789276\n",
            "train loss:  0.2902580499649048\n",
            "train loss:  0.3099743127822876\n",
            "train loss:  0.33440592885017395\n",
            "train loss:  0.3757845461368561\n",
            "train loss:  0.333181232213974\n",
            "train loss:  0.4197743535041809\n",
            "train loss:  0.18392978608608246\n",
            "train loss:  0.3967309892177582\n",
            "train loss:  0.1924830973148346\n",
            "train loss:  0.3216858208179474\n",
            "train loss:  0.335852712392807\n",
            "train loss:  0.13688744604587555\n",
            "train loss:  0.29213765263557434\n",
            "train loss:  0.2728090286254883\n",
            "train loss:  0.13158507645130157\n",
            "train loss:  0.1497875303030014\n",
            "train loss:  0.25258204340934753\n",
            "train loss:  0.2344566136598587\n",
            "train loss:  0.17825081944465637\n",
            "train loss:  0.18092571198940277\n",
            "train loss:  0.29766061902046204\n",
            "train loss:  0.21909497678279877\n",
            "train loss:  0.26267945766448975\n",
            "train loss:  0.07831819355487823\n",
            "train loss:  0.29504117369651794\n",
            "train loss:  0.19255150854587555\n",
            "train loss:  0.09479812532663345\n",
            "train loss:  0.11259926855564117\n",
            "train loss:  0.14090824127197266\n",
            "train loss:  0.29539310932159424\n",
            "train loss:  0.3205169439315796\n",
            "train loss:  0.3382323086261749\n",
            "train loss:  0.2181524932384491\n",
            "train loss:  0.26572978496551514\n",
            "train loss:  0.2391606718301773\n",
            "train loss:  0.28833332657814026\n",
            "train loss:  0.1412593573331833\n",
            "train loss:  0.2484121024608612\n",
            "train loss:  0.21263763308525085\n",
            "train loss:  0.07918751984834671\n",
            "train loss:  0.3819252848625183\n",
            "train loss:  0.2928152084350586\n",
            "train loss:  0.19341596961021423\n",
            "train loss:  0.33171215653419495\n",
            "train loss:  0.15304505825042725\n",
            "train loss:  0.175505593419075\n",
            "train loss:  0.24425533413887024\n",
            "train loss:  0.1537330001592636\n",
            "train loss:  0.23311349749565125\n",
            "train loss:  0.25053486227989197\n",
            "train loss:  0.3667600154876709\n",
            "train loss:  0.2972026765346527\n",
            "train loss:  0.4235837459564209\n",
            "train loss:  0.283145934343338\n",
            "train loss:  0.16390153765678406\n",
            "train loss:  0.11405673623085022\n",
            "train loss:  0.2017868459224701\n",
            "train loss:  0.19918504357337952\n",
            "train loss:  0.12831997871398926\n",
            "train loss:  0.24461784958839417\n",
            "train loss:  0.2966054081916809\n",
            "train loss:  0.23682770133018494\n",
            "train loss:  0.08431947976350784\n",
            "train loss:  0.33100637793540955\n",
            "train loss:  0.24614568054676056\n",
            "train loss:  0.19647318124771118\n",
            "train loss:  0.24849504232406616\n",
            "train loss:  0.18296310305595398\n",
            "train loss:  0.21763688325881958\n",
            "train loss:  0.16440655291080475\n",
            "train loss:  0.18776974081993103\n",
            "train loss:  0.3723371624946594\n",
            "train loss:  0.2045680582523346\n",
            "train loss:  0.19971594214439392\n",
            "train loss:  0.28219088912010193\n",
            "train loss:  0.24163775146007538\n",
            "train loss:  0.3212500512599945\n",
            "train loss:  0.3309934139251709\n",
            "train loss:  0.11757706850767136\n",
            "train loss:  0.3530013859272003\n",
            "train loss:  0.24621441960334778\n",
            "train loss:  0.29618218541145325\n",
            "train loss:  0.23974493145942688\n",
            "train loss:  0.19413898885250092\n",
            "train loss:  0.26530542969703674\n",
            "train loss:  0.0879731997847557\n",
            "train loss:  0.29715585708618164\n",
            "train loss:  0.2574879825115204\n",
            "train loss:  0.24048028886318207\n",
            "train loss:  0.46016278862953186\n",
            "train loss:  0.3322148323059082\n",
            "train loss:  0.427346795797348\n",
            "train loss:  0.16513341665267944\n",
            "train loss:  0.19545097649097443\n",
            "train loss:  0.13902625441551208\n",
            "train loss:  0.29378047585487366\n",
            "train loss:  0.36543676257133484\n",
            "train loss:  0.2185075581073761\n",
            "train loss:  0.20101863145828247\n",
            "train loss:  0.2857188880443573\n",
            "train loss:  0.15514619648456573\n",
            "train loss:  0.13652482628822327\n",
            "train loss:  0.27479201555252075\n",
            "train loss:  0.2770569622516632\n",
            "train loss:  0.2566331923007965\n",
            "train loss:  0.1883114129304886\n",
            "train loss:  0.3773878514766693\n",
            "train loss:  0.16866976022720337\n",
            "train loss:  0.2588086724281311\n",
            "train loss:  0.21189777553081512\n",
            "train loss:  0.20300301909446716\n",
            "train loss:  0.26708510518074036\n",
            "train loss:  0.14969545602798462\n",
            "train loss:  0.22380399703979492\n",
            "train loss:  0.16132071614265442\n",
            "train loss:  0.18343712389469147\n",
            "train loss:  0.1917123943567276\n",
            "train loss:  0.24751384556293488\n",
            "train loss:  0.35435977578163147\n",
            "train loss:  0.26388654112815857\n",
            "train loss:  0.11694150418043137\n",
            "train loss:  0.24730969965457916\n",
            "train loss:  0.1951245814561844\n",
            "train loss:  0.20458635687828064\n",
            "train loss:  0.2945765256881714\n",
            "train loss:  0.2331509292125702\n",
            "train loss:  0.32541537284851074\n",
            "train loss:  0.29911208152770996\n",
            "train loss:  0.18583758175373077\n",
            "train loss:  0.6812836527824402\n",
            "train loss:  0.11648807674646378\n",
            "train loss:  0.1521640121936798\n",
            "train loss:  0.13738538324832916\n",
            "train loss:  0.25429993867874146\n",
            "train loss:  0.37167036533355713\n",
            "train loss:  0.131349116563797\n",
            "train loss:  0.3198675215244293\n",
            "train loss:  0.18682770431041718\n",
            "train loss:  0.1092987135052681\n",
            "train loss:  0.28762027621269226\n",
            "train loss:  0.23201605677604675\n",
            "train loss:  0.1901397705078125\n",
            "train loss:  0.15287858247756958\n",
            "train loss:  0.24791304767131805\n",
            "train loss:  0.14230132102966309\n",
            "train loss:  0.09087797999382019\n",
            "train loss:  0.16510042548179626\n",
            "train loss:  0.21299074590206146\n",
            "train loss:  0.4020896852016449\n",
            "train loss:  0.6072196960449219\n",
            "train loss:  0.21073147654533386\n",
            "train loss:  0.22714491188526154\n",
            "train loss:  0.20051133632659912\n",
            "train loss:  0.31426289677619934\n",
            "train loss:  0.1995132863521576\n",
            "train loss:  0.23959583044052124\n",
            "train loss:  0.3040616810321808\n",
            "train loss:  0.24361908435821533\n",
            "train loss:  0.17344191670417786\n",
            "train loss:  0.36202964186668396\n",
            "train loss:  0.214970201253891\n",
            "train loss:  0.2727218568325043\n",
            "train loss:  0.19614160060882568\n",
            "train loss:  0.43514683842658997\n",
            "train loss:  0.27028876543045044\n",
            "train loss:  0.2551162540912628\n",
            "train loss:  0.22659097611904144\n",
            "train loss:  0.23134580254554749\n",
            "train loss:  0.1678611934185028\n",
            "train loss:  0.18396571278572083\n",
            "train loss:  0.1529313027858734\n",
            "train loss:  0.43841296434402466\n",
            "train loss:  0.25353318452835083\n",
            "train loss:  0.24390293657779694\n",
            "train loss:  0.19893915951251984\n",
            "train loss:  0.32865628600120544\n",
            "train loss:  0.2506520450115204\n",
            "train loss:  0.08604585379362106\n",
            "train loss:  0.16810689866542816\n",
            "train loss:  0.28907841444015503\n",
            "train loss:  0.26122573018074036\n",
            "train loss:  0.0772419422864914\n",
            "train loss:  0.29162341356277466\n",
            "train loss:  0.1016542986035347\n",
            "train loss:  0.23201201856136322\n",
            "train loss:  0.1532471776008606\n",
            "train loss:  0.18054640293121338\n",
            "train loss:  0.20031452178955078\n",
            "train loss:  0.17451606690883636\n",
            "train loss:  0.20703600347042084\n",
            "train loss:  0.2183472216129303\n",
            "train loss:  0.11505267024040222\n",
            "train loss:  0.15326766669750214\n",
            "train loss:  0.16389745473861694\n",
            "train loss:  0.10521198064088821\n",
            "train loss:  0.2473611980676651\n",
            "train loss:  0.3994840681552887\n",
            "train loss:  0.1667882204055786\n",
            "train loss:  0.1582038551568985\n",
            "train loss:  0.16498170793056488\n",
            "train loss:  0.2065599411725998\n",
            "train loss:  0.3896823525428772\n",
            "train loss:  0.358655720949173\n",
            "train loss:  0.20026257634162903\n",
            "train loss:  0.16738682985305786\n",
            "train loss:  0.2785041630268097\n",
            "train loss:  0.10941766202449799\n",
            "train loss:  0.14236906170845032\n",
            "train loss:  0.18300950527191162\n",
            "train loss:  0.22644513845443726\n",
            "train loss:  0.26398223638534546\n",
            "train loss:  0.21834491193294525\n",
            "train loss:  0.19210024178028107\n",
            "train loss:  0.2516697943210602\n",
            "train loss:  0.14423242211341858\n",
            "train loss:  0.10521349310874939\n",
            "train loss:  0.16506390273571014\n",
            "train loss:  0.09503696858882904\n",
            "train loss:  0.13024669885635376\n",
            "train loss:  0.09478961676359177\n",
            "train loss:  0.28284183144569397\n",
            "train loss:  0.25498875975608826\n",
            "train loss:  0.183665931224823\n",
            "train loss:  0.15933693945407867\n",
            "train loss:  0.07881952077150345\n",
            "train loss:  0.2807933986186981\n",
            "train loss:  0.16948164999485016\n",
            "train loss:  0.09193999320268631\n",
            "train loss:  0.1853386014699936\n",
            "train loss:  0.1800832748413086\n",
            "train loss:  0.28913602232933044\n",
            "train loss:  0.093592569231987\n",
            "train loss:  0.3818050026893616\n",
            "train loss:  0.30901333689689636\n",
            "train loss:  0.2303399294614792\n",
            "train loss:  0.20137754082679749\n",
            "train loss:  0.07629458606243134\n",
            "train loss:  0.0700562372803688\n",
            "train loss:  0.16279838979244232\n",
            "train loss:  0.22625194489955902\n",
            "train loss:  0.2336592823266983\n",
            "train loss:  0.16860464215278625\n",
            "train loss:  0.2318977415561676\n",
            "train loss:  0.15105657279491425\n",
            "train loss:  0.13906978070735931\n",
            "train loss:  0.2595301568508148\n",
            "train loss:  0.06454692780971527\n",
            "train loss:  0.2157503217458725\n",
            "train loss:  0.10261254757642746\n",
            "train loss:  0.21226562559604645\n",
            "train loss:  0.11982027441263199\n",
            "train loss:  0.1593465507030487\n",
            "train loss:  0.1722913384437561\n",
            "train loss:  0.11755107343196869\n",
            "train loss:  0.2513834238052368\n",
            "train loss:  0.09770435094833374\n",
            "train loss:  0.06343244761228561\n",
            "train loss:  0.20698770880699158\n",
            "train loss:  0.20296597480773926\n",
            "train loss:  0.1736038625240326\n",
            "train loss:  0.23005512356758118\n",
            "train loss:  0.1052810475230217\n",
            "train loss:  0.20985068380832672\n",
            "train loss:  0.15424463152885437\n",
            "train loss:  0.15118268132209778\n",
            "train loss:  0.2612563967704773\n",
            "train loss:  0.17738263309001923\n",
            "train loss:  0.1880815029144287\n",
            "train loss:  0.08932238072156906\n",
            "train loss:  0.16310597956180573\n",
            "train loss:  0.1542225480079651\n",
            "train loss:  0.1193842813372612\n",
            "train loss:  0.15825578570365906\n",
            "train loss:  0.1698613464832306\n",
            "train loss:  0.11061413586139679\n",
            "train loss:  0.1262727677822113\n",
            "train loss:  0.16768287122249603\n",
            "train loss:  0.1942754089832306\n",
            "train loss:  0.06949625164270401\n",
            "train loss:  0.13355880975723267\n",
            "train loss:  0.1497178077697754\n",
            "train loss:  0.2660546600818634\n",
            "train loss:  0.333890825510025\n",
            "train loss:  0.12311887741088867\n",
            "train loss:  0.19081170856952667\n",
            "train loss:  0.13807061314582825\n",
            "train loss:  0.10349785536527634\n",
            "train loss:  0.13771115243434906\n",
            "train loss:  0.23869729042053223\n",
            "train loss:  0.0822935476899147\n",
            "train loss:  0.14765866100788116\n",
            "train loss:  0.30349454283714294\n",
            "train loss:  0.2146793156862259\n",
            "train loss:  0.11741048842668533\n",
            "train loss:  0.28308913111686707\n",
            "train loss:  0.045430999249219894\n",
            "train loss:  0.18848218023777008\n",
            "train loss:  0.09760844707489014\n",
            "train loss:  0.11964825540781021\n",
            "train loss:  0.23216325044631958\n",
            "train loss:  0.28188711404800415\n",
            "train loss:  0.12526340782642365\n",
            "train loss:  0.16253936290740967\n",
            "train loss:  0.297504723072052\n",
            "train loss:  0.3546575903892517\n",
            "train loss:  0.23767898976802826\n",
            "train loss:  0.12984412908554077\n",
            "train loss:  0.09707558900117874\n",
            "train loss:  0.189248189330101\n",
            "train loss:  0.25290587544441223\n",
            "train loss:  0.27370238304138184\n",
            "train loss:  0.15317581593990326\n",
            "train loss:  0.17196805775165558\n",
            "train loss:  0.06911884993314743\n",
            "train loss:  0.15798844397068024\n",
            "train loss:  0.4451861083507538\n",
            "train loss:  0.2846384346485138\n",
            "train loss:  0.11206088215112686\n",
            "train loss:  0.2977016270160675\n",
            "train loss:  0.19085127115249634\n",
            "train loss:  0.13465361297130585\n",
            "train loss:  0.2035931944847107\n",
            "train loss:  0.08126737922430038\n",
            "train loss:  0.045266084372997284\n",
            "train loss:  0.1958998292684555\n",
            "train loss:  0.37409716844558716\n",
            "train loss:  0.1728271245956421\n",
            "train loss:  0.2307925820350647\n",
            "train loss:  0.11384180188179016\n",
            "train loss:  0.50173020362854\n",
            "train loss:  0.05395645275712013\n",
            "train loss:  0.12664547562599182\n",
            "train loss:  0.20236928761005402\n",
            "train loss:  0.07842268794775009\n",
            "train loss:  0.2351328581571579\n",
            "train loss:  0.05227125063538551\n",
            "train loss:  0.163601353764534\n",
            "train loss:  0.16995252668857574\n",
            "train loss:  0.20477589964866638\n",
            "train loss:  0.043911125510931015\n",
            "train loss:  0.18049466609954834\n",
            "train loss:  0.15690366923809052\n",
            "train loss:  0.22778555750846863\n",
            "train loss:  0.11256816983222961\n",
            "train loss:  0.2738784849643707\n",
            "train loss:  0.23452851176261902\n",
            "train loss:  0.28359246253967285\n",
            "train loss:  0.13269226253032684\n",
            "train loss:  0.2692796289920807\n",
            "train loss:  0.1594613492488861\n",
            "train loss:  0.1662115752696991\n",
            "train loss:  0.13282126188278198\n",
            "train loss:  0.060410939157009125\n",
            "train loss:  0.28799161314964294\n",
            "train loss:  0.26622599363327026\n",
            "train loss:  0.24574127793312073\n",
            "train loss:  0.26567408442497253\n",
            "train loss:  0.26666200160980225\n",
            "train loss:  0.09600193053483963\n",
            "train loss:  0.16443008184432983\n",
            "train loss:  0.1681412011384964\n",
            "train loss:  0.13071948289871216\n",
            "train loss:  0.2370775043964386\n",
            "train loss:  0.14144538342952728\n",
            "train loss:  0.20885223150253296\n",
            "train loss:  0.24062158167362213\n",
            "train loss:  0.04541805386543274\n",
            "train loss:  0.15806953608989716\n",
            "train loss:  0.07733870297670364\n",
            "train loss:  0.16457253694534302\n",
            "train loss:  0.238555446267128\n",
            "train loss:  0.07083600759506226\n",
            "train loss:  0.27355071902275085\n",
            "train loss:  0.17592206597328186\n",
            "train loss:  0.27498286962509155\n",
            "train loss:  0.26610836386680603\n",
            "train loss:  0.2633000612258911\n",
            "train loss:  0.11667361110448837\n",
            "train loss:  0.26127657294273376\n",
            "train loss:  0.21793900430202484\n",
            "train loss:  0.13052663207054138\n",
            "train loss:  0.42183661460876465\n",
            "train loss:  0.060260891914367676\n",
            "train loss:  0.23039692640304565\n",
            "train loss:  0.18137866258621216\n",
            "train loss:  0.18583521246910095\n",
            "train loss:  0.31075337529182434\n",
            "train loss:  0.05130487307906151\n",
            "train loss:  0.17770910263061523\n",
            "train loss:  0.19207720458507538\n",
            "train loss:  0.12414093315601349\n",
            "train loss:  0.19262489676475525\n",
            "train loss:  0.15400992333889008\n",
            "train loss:  0.1855376958847046\n",
            "train loss:  0.17326147854328156\n",
            "train loss:  0.1472236067056656\n",
            "train loss:  0.04656175523996353\n",
            "train loss:  0.15158315002918243\n",
            "train loss:  0.1840241253376007\n",
            "train loss:  0.1580723375082016\n",
            "train loss:  0.2662949860095978\n",
            "train loss:  0.1593916416168213\n",
            "train loss:  0.15811973810195923\n",
            "train loss:  0.2053983062505722\n",
            "train loss:  0.1731458157300949\n",
            "train loss:  0.06814906001091003\n",
            "train loss:  0.19500137865543365\n",
            "train loss:  0.1252387911081314\n",
            "train loss:  0.12901350855827332\n",
            "train loss:  0.19839565455913544\n",
            "train loss:  0.18161219358444214\n",
            "train loss:  0.08286163210868835\n",
            "train loss:  0.26412707567214966\n",
            "train loss:  0.07198336720466614\n",
            "train loss:  0.1820281594991684\n",
            "train loss:  0.21802258491516113\n",
            "train loss:  0.14396171271800995\n",
            "train loss:  0.05815340206027031\n",
            "train loss:  0.11701713502407074\n",
            "train loss:  0.09978210926055908\n",
            "train loss:  0.1852770745754242\n",
            "train loss:  0.11444443464279175\n",
            "train loss:  0.06113707274198532\n",
            "train loss:  0.1384430080652237\n",
            "train loss:  0.07751467078924179\n",
            "train loss:  0.39225250482559204\n",
            "train loss:  0.054444294422864914\n",
            "train loss:  0.2183535248041153\n",
            "train loss:  0.16156987845897675\n",
            "train loss:  0.15067893266677856\n",
            "train loss:  0.2573654353618622\n",
            "train loss:  0.1667727530002594\n",
            "train loss:  0.06758584827184677\n",
            "train loss:  0.07904890179634094\n",
            "train loss:  0.053458645939826965\n",
            "train loss:  0.18001721799373627\n",
            "train loss:  0.2710184156894684\n",
            "train loss:  0.09288371354341507\n",
            "train loss:  0.10972749441862106\n",
            "train loss:  0.09656444936990738\n",
            "train loss:  0.30222779512405396\n",
            "train loss:  0.17557956278324127\n",
            "train loss:  0.14343520998954773\n",
            "train loss:  0.044938310980796814\n",
            "train loss:  0.32882019877433777\n",
            "train loss:  0.07262194156646729\n",
            "train loss:  0.054688483476638794\n",
            "train loss:  0.04709310084581375\n",
            "train loss:  0.21956248581409454\n",
            "train loss:  0.14513181149959564\n",
            "train loss:  0.15125277638435364\n",
            "train loss:  0.11973021924495697\n",
            "train loss:  0.3213973343372345\n",
            "train loss:  0.0877448171377182\n",
            "train loss:  0.14571340382099152\n",
            "train loss:  0.33419862389564514\n",
            "train loss:  0.06830921769142151\n",
            "train loss:  0.20748373866081238\n",
            "train loss:  0.23630623519420624\n",
            "train loss:  0.08693745732307434\n",
            "train loss:  0.18374769389629364\n",
            "train loss:  0.07058563828468323\n",
            "train loss:  0.2214398980140686\n",
            "train loss:  0.0713675245642662\n",
            "train loss:  0.20019832253456116\n",
            "train loss:  0.1842861771583557\n",
            "train loss:  0.16787301003932953\n",
            "train loss:  0.1501777172088623\n",
            "train loss:  0.07040058821439743\n",
            "train loss:  0.0924229621887207\n",
            "train loss:  0.09899913519620895\n",
            "train loss:  0.17525282502174377\n",
            "train loss:  0.18606790900230408\n",
            "train loss:  0.1547958254814148\n",
            "train loss:  0.13074041903018951\n",
            "train loss:  0.260225385427475\n",
            "train loss:  0.18981888890266418\n",
            "train loss:  0.057364627718925476\n",
            "train loss:  0.05891386792063713\n",
            "train loss:  0.3258178234100342\n",
            "train loss:  0.07833041250705719\n",
            "train loss:  0.4214894771575928\n",
            "train loss:  0.20228968560695648\n",
            "train loss:  0.08991603553295135\n",
            "train loss:  0.2814284861087799\n",
            "train loss:  0.22746509313583374\n",
            "train loss:  0.08714371919631958\n",
            "train loss:  0.25285398960113525\n",
            "train loss:  0.12274949252605438\n",
            "train loss:  0.1567683219909668\n",
            "train loss:  0.1768450140953064\n",
            "train loss:  0.11614367365837097\n",
            "train loss:  0.19464513659477234\n",
            "train loss:  0.05230963975191116\n",
            "train loss:  0.1363668292760849\n",
            "train loss:  0.2049572616815567\n",
            "train loss:  0.072398841381073\n",
            "train loss:  0.11084157228469849\n",
            "train loss:  0.17948414385318756\n",
            "train loss:  0.08305609971284866\n",
            "train loss:  0.1822630763053894\n",
            "train loss:  0.09466689825057983\n",
            "train loss:  0.025928497314453125\n",
            "train loss:  0.1751253604888916\n",
            "train loss:  0.22842098772525787\n",
            "train loss:  0.0853978618979454\n",
            "train loss:  0.06296642124652863\n",
            "train loss:  0.12901172041893005\n",
            "train loss:  0.1618187129497528\n",
            "train loss:  0.21830612421035767\n",
            "train loss:  0.1414400190114975\n",
            "train loss:  0.20633980631828308\n",
            "train loss:  0.18096278607845306\n",
            "train loss:  0.3174551725387573\n",
            "train loss:  0.11203540116548538\n",
            "train loss:  0.08959734439849854\n",
            "train loss:  0.17561247944831848\n",
            "train loss:  0.07251325249671936\n",
            "train loss:  0.15217749774456024\n",
            "train loss:  0.21960949897766113\n",
            "train loss:  0.06632599979639053\n",
            "train loss:  0.09047797322273254\n",
            "train loss:  0.2154819220304489\n",
            "train loss:  0.16713860630989075\n",
            "train loss:  0.2486705183982849\n",
            "train loss:  0.22803781926631927\n",
            "train loss:  0.38542482256889343\n",
            "train loss:  0.19425176084041595\n",
            "train loss:  0.03685920313000679\n",
            "train loss:  0.08853799104690552\n",
            "train loss:  0.10805075615644455\n",
            "train loss:  0.07920574396848679\n",
            "train loss:  0.14522410929203033\n",
            "train loss:  0.14487531781196594\n",
            "train loss:  0.1516529619693756\n",
            "train loss:  0.18269403278827667\n",
            "train loss:  0.20139460265636444\n",
            "train loss:  0.22680824995040894\n",
            "train loss:  0.06678438186645508\n",
            "train loss:  0.08669371157884598\n",
            "train loss:  0.10895989090204239\n",
            "train loss:  0.16150033473968506\n",
            "train loss:  0.1995963752269745\n",
            "train loss:  0.13072572648525238\n",
            "train loss:  0.21894872188568115\n",
            "train loss:  0.10172220319509506\n",
            "train loss:  0.1869249939918518\n",
            "train loss:  0.19184750318527222\n",
            "train loss:  0.13413174450397491\n",
            "train loss:  0.11988608539104462\n",
            "train loss:  0.12116687744855881\n",
            "train loss:  0.04272511228919029\n",
            "train loss:  0.169855996966362\n",
            "train loss:  0.07155349850654602\n",
            "train loss:  0.19487138092517853\n",
            "train loss:  0.16807158291339874\n",
            "train loss:  0.2814658582210541\n",
            "train loss:  0.22543558478355408\n",
            "train loss:  0.12494359910488129\n",
            "train loss:  0.1747518926858902\n",
            "train loss:  0.12391404062509537\n",
            "train loss:  0.3929003179073334\n",
            "train loss:  0.19962763786315918\n",
            "train loss:  0.21620342135429382\n",
            "train loss:  0.35413858294487\n",
            "train loss:  0.1425827443599701\n",
            "train loss:  0.09175010770559311\n",
            "train loss:  0.22797070443630219\n",
            "train loss:  0.23535460233688354\n",
            "train loss:  0.09602769464254379\n",
            "train loss:  0.18532603979110718\n",
            "train loss:  0.18766933679580688\n",
            "train loss:  0.21556739509105682\n",
            "train loss:  0.16864041984081268\n",
            "train loss:  0.17845138907432556\n",
            "train loss:  0.10701743513345718\n",
            "train loss:  0.22894936800003052\n",
            "train loss:  0.2510252296924591\n",
            "train loss:  0.22279280424118042\n",
            "train loss:  0.11337650567293167\n",
            "train loss:  0.05595266819000244\n",
            "train loss:  0.4729079604148865\n",
            "train loss:  0.18662795424461365\n",
            "train loss:  0.06935019046068192\n",
            "train loss:  0.1672152727842331\n",
            "train loss:  0.047803279012441635\n",
            "train loss:  0.17619626224040985\n",
            "train loss:  0.10396227985620499\n",
            "train loss:  0.12992903590202332\n",
            "train loss:  0.018346790224313736\n",
            "train loss:  0.1535714715719223\n",
            "train loss:  0.18240056931972504\n",
            "train loss:  0.20423825085163116\n",
            "train loss:  0.07254453748464584\n",
            "train loss:  0.36061620712280273\n",
            "train loss:  0.05727357044816017\n",
            "train loss:  0.09322026371955872\n",
            "train loss:  0.08266019076108932\n",
            "train loss:  0.239653080701828\n",
            "train loss:  0.1637527346611023\n",
            "train loss:  0.22415584325790405\n",
            "train loss:  0.18359903991222382\n",
            "train loss:  0.07915672659873962\n",
            "train loss:  0.08797940611839294\n",
            "train loss:  0.06528562307357788\n",
            "train loss:  0.1827903538942337\n",
            "train loss:  0.38000717759132385\n",
            "train loss:  0.08644374459981918\n",
            "train loss:  0.11712589114904404\n",
            "train loss:  0.1407773643732071\n",
            "train loss:  0.20771783590316772\n",
            "train loss:  0.02253386750817299\n",
            "train loss:  0.04096086695790291\n",
            "train loss:  0.09578737616539001\n",
            "train loss:  0.04003407433629036\n",
            "train loss:  0.1924082487821579\n",
            "train loss:  0.07311341166496277\n",
            "train loss:  0.20935696363449097\n",
            "train loss:  0.1883375495672226\n",
            "train loss:  0.1609378606081009\n",
            "train loss:  0.042276717722415924\n",
            "train loss:  0.15920497477054596\n",
            "train loss:  0.09256535768508911\n",
            "train loss:  0.2048914134502411\n",
            "train loss:  0.3820011615753174\n",
            "train loss:  0.18515391647815704\n",
            "train loss:  0.19369959831237793\n",
            "train loss:  0.11476343870162964\n",
            "train loss:  0.16083332896232605\n",
            "train loss:  0.06237266585230827\n",
            "train loss:  0.11372549831867218\n",
            "train loss:  0.11877679079771042\n",
            "train loss:  0.07905249297618866\n",
            "train loss:  0.34413787722587585\n",
            "train loss:  0.18314418196678162\n",
            "train loss:  0.14323991537094116\n",
            "train loss:  0.11113568395376205\n",
            "train loss:  0.06870799511671066\n",
            "train loss:  0.08402444422245026\n",
            "train loss:  0.1437433660030365\n",
            "train loss:  0.1744256317615509\n",
            "train loss:  0.04237348958849907\n",
            "train loss:  0.26131099462509155\n",
            "train loss:  0.1441585272550583\n",
            "train loss:  0.03578395023941994\n",
            "train loss:  0.07346153259277344\n",
            "train loss:  0.1014045849442482\n",
            "train loss:  0.12154482305049896\n",
            "train loss:  0.14809244871139526\n",
            "train loss:  0.07723775506019592\n",
            "train loss:  0.21814511716365814\n",
            "train loss:  0.15864799916744232\n",
            "train loss:  0.16751894354820251\n",
            "train loss:  0.06041230633854866\n",
            "train loss:  0.20171701908111572\n",
            "train loss:  0.3427221477031708\n",
            "train loss:  0.1329556554555893\n",
            "val_loss:  0.125268816947937\n"
          ]
        }
      ]
    }
  ]
}